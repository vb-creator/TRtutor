{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4736,
     "status": "ok",
     "timestamp": 1747245034042,
     "user": {
      "displayName": "Chinmai Anandh Chappa",
      "userId": "16938135707168635898"
     },
     "user_tz": 240
    },
    "id": "Aj_bTiczz5YG",
    "outputId": "0696275a-225c-43eb-c82c-7e7ffc0302c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.11/dist-packages (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.11/dist-packages (0.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "# prompt: I am using these from dataclasses import dataclass, field\n",
    "# from typing import List\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftConfig, PeftModel\n",
    "# from tqdm import tqdm\n",
    "# import openai\n",
    "# import json\n",
    "# import os, random\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "# import pandas as pd\n",
    "# install all of em\n",
    "\n",
    "!pip install torch transformers peft tqdm openai pandas google-generativeai typing dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6441,
     "status": "ok",
     "timestamp": 1747245666645,
     "user": {
      "displayName": "Chinmai Anandh Chappa",
      "userId": "16938135707168635898"
     },
     "user_tz": 240
    },
    "id": "Yda-CI2385Aw",
    "outputId": "8bf43e89-26a3-4bfc-92b7-8dd5a8a3d364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  qwen_7b_checkpoint_150.zip\n",
      "   creating: Saved_Models/qwen_7b_checkpoint_150/\n",
      "  inflating: Saved_Models/__MACOSX/._qwen_7b_checkpoint_150  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/adapter_model.safetensors  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._adapter_model.safetensors  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/added_tokens.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._added_tokens.json  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/rng_state.pth  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._rng_state.pth  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/tokenizer_config.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._tokenizer_config.json  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/special_tokens_map.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._special_tokens_map.json  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/optimizer.pt  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._optimizer.pt  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/scheduler.pt  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._scheduler.pt  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/tokenizer.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._tokenizer.json  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/README.md  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._README.md  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/merges.txt  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._merges.txt  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/training_args.bin  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._training_args.bin  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/adapter_config.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._adapter_config.json  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/vocab.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._vocab.json  \n",
      "  inflating: Saved_Models/qwen_7b_checkpoint_150/trainer_state.json  \n",
      "  inflating: Saved_Models/__MACOSX/qwen_7b_checkpoint_150/._trainer_state.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"qwen_7b_checkpoint_150.zip\" -d \"Saved_Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1747252970142,
     "user": {
      "displayName": "Chinmai Anandh Chappa",
      "userId": "16938135707168635898"
     },
     "user_tz": 240
    },
    "id": "WlpuwcjhzvHW"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import json\n",
    "import os, random\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "TALKER_PROMPT = Path(\"talker_prompt.txt\").read_text()\n",
    "REASONER_PROMPT = Path(\"reasoner_prompt.txt\").read_text()\n",
    "\n",
    "@dataclass\n",
    "class Talker:\n",
    "    model_path: str\n",
    "    prompt_template: str\n",
    "\n",
    "    def __init__(self, model_path: str, prompt_template: str):\n",
    "        self.model_path = model_path\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        # Load adapter configuration and base model\n",
    "        cfg = PeftConfig.from_pretrained(model_path)\n",
    "        base = cfg.base_model_name_or_path\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\", use_fast=True)\n",
    "\n",
    "        # Load base model and apply LoRA adapter\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "        )\n",
    "        offload_dir = \"./offload\"\n",
    "        model = PeftModel.from_pretrained(model, model_path, torch_dtype=torch.float16, offload_folder=offload_dir)\n",
    "\n",
    "        # Merge weights for speed and lower memory usage\n",
    "        model = model.merge_and_unload().eval()\n",
    "\n",
    "        # Assign tokenizer and model to instance variables\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def respond(self, student_persona: dict, reasoner_context: dict, conversation_history: List[dict]) -> str:\n",
    "        # Create the prompt for the talker\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            student_persona=student_persona,\n",
    "            reasoner_context=reasoner_context,\n",
    "            conversation_history=conversation_history\n",
    "        )\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        tokens = self.tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        # Generate response using the model\n",
    "        gen = self.model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens=80,  # You can adjust this value as needed\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens to get the response\n",
    "        new_ids = gen[0][tokens[\"input_ids\"].shape[-1]:]\n",
    "        response = self.tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "@dataclass\n",
    "class Reasoner:\n",
    "    api_key: str\n",
    "    model: str = \"gpt-3.5-turbo\"\n",
    "    prompt_template: str = REASONER_PROMPT\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\", prompt_template = REASONER_PROMPT):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        openai.api_key = self.api_key\n",
    "        self.client = openai.OpenAI(api_key = api_key)\n",
    "\n",
    "    def reason(\n",
    "            self,\n",
    "            question: str,\n",
    "               student_persona: dict, conversation_history: List[dict], prev_reasoner_context: dict, max_tokens: int = 150) -> dict:\n",
    "        # Create the prompt for the reasoner\n",
    "        prompt = self.prompt_template.format(\n",
    "        QUESTION=question,\n",
    "        STUDENT_PERSONA=json.dumps(student_persona, ensure_ascii=False),\n",
    "        CONVERSATION_HISTORY=json.dumps(conversation_history, ensure_ascii=False),\n",
    "        PREV_REASONER_CONTEXT=json.dumps(prev_reasoner_context, ensure_ascii=False)\n",
    "        )\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            return json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Failed to parse the response into JSON.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error: {str(e)}\"}\n",
    "\n",
    "@dataclass\n",
    "class Tutor:\n",
    "    talker: Talker\n",
    "    reasoner: Reasoner\n",
    "    question: str\n",
    "    student_persona: dict\n",
    "    reasoner_context: dict\n",
    "    conversation_history: List[dict] = field(default_factory=list)\n",
    "    turn_counter: int = 0\n",
    "\n",
    "    def student_respond(self, msg: str):\n",
    "        self.conversation_history.append({\"speaker\": \"student\", \"text\": msg})\n",
    "        self.turn_counter += 1\n",
    "\n",
    "    def teacher_respond(self) -> str:\n",
    "        reply = self.talker.respond(self.student_persona, self.reasoner_context, self.conversation_history)\n",
    "        self.conversation_history.append({\"speaker\": \"teacher\", \"text\": reply})\n",
    "        self.turn_counter += 1\n",
    "        return reply\n",
    "\n",
    "    def update_reasoner_context(self):\n",
    "        if not (self.turn_counter > 0 and self.turn_counter % 4 == 0):\n",
    "            updated_context = self.reasoner.reason(\n",
    "                question=self.question,\n",
    "                student_persona=self.student_persona,\n",
    "                conversation_history=self.conversation_history,\n",
    "                prev_reasoner_context=self.reasoner_context\n",
    "            )\n",
    "            if \"error\" not in updated_context:\n",
    "                self.reasoner_context.update(updated_context)\n",
    "            else:\n",
    "                print(f\"Couldn't update Reasoner context. Reasoner Error: {updated_context['error']}\")\n",
    "\n",
    "\n",
    "class Student:\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")  # ← set via env var\n",
    "    MODEL_NAME = \"gemini-2.0-flash\"\n",
    "    MAX_TURNS = 8  # teacher+student turns per question\n",
    "    STUDENT_MISTAKE_PROB = 1  # 30 % chance student makes a wrong calc\n",
    "\n",
    "    client = genai.Client(api_key=\"\")\n",
    "    def gen_student_prompt(self, student_persona: dict, history: List[dict]) -> str:\n",
    "        history = [f\"{msg['speaker']}: {msg['text']}\" for msg in history]\n",
    "        history_block = \"\\n\".join(history)\n",
    "        mistake_clause = \" but this time do the calculation wrong.\" if random.random() < self.STUDENT_MISTAKE_PROB else \".\"\n",
    "\n",
    "        return f\"\"\"\n",
    "        You are a student trying to solve a problem with your teacher.\n",
    "\n",
    "        Your profile:\n",
    "        {student_persona}\n",
    "\n",
    "        Conversation so far:\n",
    "        {history_block}\n",
    "\n",
    "        Respond to the teacher with a single message{mistake_clause}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def generate_with_gemini(self, prompt: str) -> str:\n",
    "        \"\"\"Stream‑generate a completion from Gemini and return the full text.\"\"\"\n",
    "\n",
    "        contents = [types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt)])]\n",
    "        cfg = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
    "\n",
    "        result = \"\"\n",
    "        for chunk in self.client.models.generate_content_stream(model=self.MODEL_NAME, contents=contents, config=cfg):\n",
    "            result += chunk.text\n",
    "        return result.strip()\n",
    "\n",
    "    def generate_student_dialogue(self, student_persona, history):\n",
    "        prompt = self.gen_student_prompt(student_persona, history)\n",
    "        response = self.generate_with_gemini(prompt)\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "def start_conversation(tutor: Tutor, question: str, student_persona: dict, student_model: Student = None):\n",
    "    tutor.question = question\n",
    "    tutor.student_persona = student_persona\n",
    "    tutor.reasoner_context = {\"belief_state\": {}, \"chain_of_thought\": \"\", \"final_answer\": \"\"}\n",
    "    tutor.conversation_history = []\n",
    "    tutor.turn_counter = 0\n",
    "\n",
    "    student_prompt = question\n",
    "    tutor.student_respond(student_prompt)\n",
    "    tutor.update_reasoner_context()\n",
    "    print(f\"Student: {student_prompt}\")\n",
    "    while tutor.turn_counter < Tutor.MAX_TURNS:\n",
    "        teacher_reply = tutor.teacher_respond()\n",
    "        print(f\"Teacher: {teacher_reply}\")\n",
    "        tutor.update_reasoner_context()\n",
    "\n",
    "        if student_model is None:\n",
    "            student_prompt = input(\"Student: \")\n",
    "        else:\n",
    "            student_prompt = student_model.generate_student_dialogue(student_persona, tutor.conversation_history)\n",
    "            print(f\"Student: {student_prompt}\")\n",
    "            if student_prompt.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Exiting conversation.\")\n",
    "                break\n",
    "        tutor.student_respond(student_prompt)\n",
    "\n",
    "\n",
    "# TALKER_PROMPT = \"\"\"\n",
    "# You are a teacher helping a student with math problems.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3142,
     "status": "ok",
     "timestamp": 1747252975912,
     "user": {
      "displayName": "Chinmai Anandh Chappa",
      "userId": "16938135707168635898"
     },
     "user_tz": 240
    },
    "id": "IuaJer31zvHY",
    "outputId": "6d2e7a0c-e947-41ea-9dd0-68ff04fcb002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Question: There are one-third as many Ford trucks as Dodge trucks in the parking lot of the Taco Castle.  But there are twice as many Ford trucks as Toyota trucks in this parking lot, and there are half as many Volkswagen Bugs as there are Toyota trucks in this same parking lot.  If there are 5 Volkswagon Bugs in the parking lot, how many Dodge trucks are in the parking lot of the Taco Castle?\n",
      "Student Persona: {'name': 'Luca', 'gender': 'female', 'interests': 'drawing, music, nature walks', 'personality': 'quiet, creative, observant', 'background': 'expresses herself better through art and visuals than words; enjoys exploring ideas through creative storytelling.'}\n",
      "--------------------------------------------------\n",
      "Reasoner Context :{'belief_state': 'Luca seems to understand the problem statement and has not shown any confusion so far.', 'chain_of_thought': 'Not provided yet.', 'final_answer': 'Not provided yet.', 'update': False}\n",
      "Student: Okay, so if there are 5 Volkswagen Bugs, and that's half the number of Toyota trucks, then there must be 10 Toyota trucks. And since there are twice as many Ford trucks as Toyota trucks, that means there are 20 Ford trucks. And because there are one-third as many Ford trucks as Dodge trucks, that means there must be 60 Dodge trucks!\n",
      "Reasoner Context :{'belief_state': 'Luca seems to have misunderstood the relationship between the number of Ford trucks and Dodge trucks. She incorrectly calculated the number of Dodge trucks based on the given information.', 'chain_of_thought': '', 'final_answer': '', 'update': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_path = \"test_mathdial.json\"\n",
    "# Load test data\n",
    "df = pd.read_json(test_data_path)\n",
    "test_data = df.to_dict(\"records\")\n",
    "# Example student persona\n",
    "# print(test_data[0].keys())\n",
    "\n",
    "first_5_rows = test_data[:1]\n",
    "for row in first_5_rows:\n",
    "    question = row[\"question\"]\n",
    "    student_persona = row[\"student_persona\"]\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Student Persona: {student_persona}\")\n",
    "    print(\"-\" * 50)\n",
    "    # Initialize Tutor components\n",
    "    # talker = Talker(model_path=\"Saved_Models/qwen_7b_checkpoint_150/\", prompt_template=TALKER_PROMPT)\n",
    "    talker = None\n",
    "    reasoner = Reasoner(api_key=\"\",\n",
    "                        prompt_template=REASONER_PROMPT)\n",
    "    tutor = Tutor(talker=talker, reasoner=reasoner, question=question, student_persona=student_persona, reasoner_context={})\n",
    "    # Start conversation\n",
    "    start_conversation(tutor, question, student_persona)\n",
    "\n",
    "    # sample code to test reasoner and student model\n",
    "    # tutor.student_respond(question)\n",
    "    # tutor.update_reasoner_context()\n",
    "    # print(f\"Reasoner Context :{tutor.reasoner_context}\")\n",
    "    # student_model = Student()\n",
    "    # student_prompt = student_model.generate_student_dialogue(student_persona, tutor.conversation_history)\n",
    "    # print(f\"Student: {student_prompt}\")\n",
    "    # tutor.student_respond(student_prompt)\n",
    "    # tutor.update_reasoner_context()\n",
    "    # print(f\"Reasoner Context :{tutor.reasoner_context}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
